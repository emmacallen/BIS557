
---
title: "homework-2"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework-2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(mc.cores = 2)
library(casl)

```



##1. CASL 2.11 Problem #5

**Consider the regression model with a scalar *x* and intercept:**
$$
y = \beta_0 + \beta_1 * x
$$
**Using the explicit formula for the inverse of a 2-by-2 matrix, write down the least squares estimators for $\hat{\beta_0}$ and $\hat{\beta_1}$.**

We can express the model in matrix form, $Y = X\beta$, where each term represents the following matrices:
$$
\mathbf{Y}_{n \times p} = \left[\begin{array}
{ccc}
y_{1} \\
...  \\
y_{n}  \\
\end{array}\right] \hspace{1cm}
\mathbf{X}_{n \times p} = \left[\begin{array}
{ccc}
1 & x_{1}\\
...  & ...\\
1 & x_{n}  \\
\end{array}\right] \hspace{1cm}
\mathbf{\beta} = \left[\begin{array}
{ccc}
\beta_{0} \\
\beta_{1}  \\
\end{array}\right]
$$


We can solve for the $\beta$s value with the following formula:
$$
\beta = (X'X)^{-1}X'Y
$$
Explicitly:

$$
X'X = \mathbf{X}_{n \times p} = \left[\begin{array}
{ccc}
n & \sum x_i \\
\sum x_i & \sum x_i^2  \\
\end{array}\right]
$$

Now we take the inverse of the above 2x2 matrix using the explicit formula:

$$
A = \left[\begin{array}
{ccc}
a & b \\
c & d  \\
\end{array}\right] \hspace{1cm} 

A' = \frac{1}{ad-bc}\left[\begin{array}
{ccc}
d & -b \\
-c & a  \\
\end{array}\right] 
$$

The inverse of $X'X$ is:

$$
(X'X)^{-1} = \frac{1}{n \sum x_i^2 - (\sum x_i)^2}\left[\begin{array}
{ccc}
\sum x_i^2 & -\sum x_i \\
-\sum x_i & n  \\
\end{array}\right]
$$
Next, we find the cross product of X and Y:
$$
X'Y = \left[\begin{array}
{ccc}
1 & ... & 1 \\
x_1 &... & x_n  \\
\end{array}\right] \left[\begin{array}
{ccc}
y_1 \\
... \\
y_n \\
\end{array}\right]=\left[\begin{array}
{ccc}
\sum y_i \\
\sum x_i y_i  \\
\end{array}\right]
$$

We can combine the two previous formulas to solve for beta:

$$
\beta = \frac{1}{n \sum x_i^2 - (\sum x_i)^2}\left[\begin{array}
{ccc}
\sum x_i^2 & -\sum x_i) \\
-\sum x_i & n  \\
\end{array}\right]\left[\begin{array}
{ccc}
\sum y_i \\
\sum x_i y_i  \\
\end{array}\right]
$$
$$
= \frac{1}{n \sum x_i^2 - (\sum x_i)^2}\left[\begin{array}
{ccc}
\sum x_i^2 \sum y_i - \sum x_i \sum x_i y_i \\
-\sum x_i \sum y_i + n \sum x_i y_i  \\
\end{array}\right]
$$

##2. Ridge Regression with Collinearity Handling 

The function `ridge_regression` returns a list of $\beta$ coefficients while handling collinearity. 

```{r}
data("iris")
iris$duplicate <- iris$Sepal.Width
ridge_regression(Sepal.Length ~ ., iris, lambda = 0)
```

##3. Cross Validation for `ridge_regression`

The function `cross_validation` provides a list containing a tibble with the RMSEs, standard error of the means, and confidence interval bounds. Additionally, it displays a plot of the lambda values tested and their corresponding RMSEs. `lambda_min`, the lambda value with the lowest RMSE, has been included. 

```{r}
data("iris")
cross_validation(Sepal.Length ~ ., iris)
```


##4.Numerical Stability, Statistical Error, and Ridge Regression
**Section 2.8 of CASL shows that as the numerical stability decreases, statistical errors increase. Reproduce the results and then show that using ridge regression can increase numerical stability and decrease statistical error.**

The following code has been adapted from section 2.8 of CASL. 
We are generating data to see how close the OLS estimate is to the true $\beta$.

```{r}
n <- 1000; p <- 25
beta <- c(1, rep(0, p-1))
X  <- matrix(rnorm(n * p), ncol = p)
svals <- svd(X)$d
max(svals)/min(svals)
N <- 1e4; `12_errors` <- rep(0, N)
for (k in 1:N){
  y <- X %*% beta + rnorm(n)
  betahat <- casl_ols_svd(X, y)
  `12_errors`[k] <- sqrt(sum((betahat - beta)^2))
}
mean(`12_errors`)
```

Replacing the first column of X with linear combination of the original first column and second column will produce an increased condition number.

```{r}
n <- 1000; p <- 25
beta <- c(1, rep(0, p-1))
X  <- matrix(rnorm(n * p), ncol = p)
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
svals <- svd(X)$d
max(svals) / min(svals)
```

Statistical errors increase as the numerical stability decreases as the bounds in this equation are the "worst-case scenerio".

We can then use Ridge Regression to show an increase in numerical stability and a decrease in statistical eror. Solving for the beta value and penalty from the previous equations, we find that the conditional number of our matrix is equal to $\sigma_{max} / \sigma_{min}$. Reducing the denominator(lower bound) of this equation will in turn reduce the statistical error. 

To compute the $\beta$: 
```{r}
X <- matrix(c(10^9, -1, -1, 10^(-5)), 2, 2)  
beta <- c(1,1)  y <- X %*% beta  
solve(X, y) 
```

We can compute the conditional number of X with the following: 
```{r}
svals <- svd(X)$d
svals
```

Arnold, Taylor. A Computational Approach to Statistical Learning (Chapman & Hall/CRC Texts in Statistical Science) (p. 27). CRC Press. Kindle Edition. 

Therefore, by introducing some bias, we can reduce the statistical error of our estimate.

##5. The LASSO Penalty

**Consider the LASSO penalty:**
$$
\frac{1}{2n} ||Y - X \beta||^2_2 + \lambda ||\beta||_1
$$

**Show that if $|X_j^T| \le n \lambda$, then $\hat{\beta}^{LASSO}$ must be zero.**
This vignette has been adapted from office hour discussions.

Assuming that the individual factors wi X are indpendent such that $||X||_2^2 = I$ and if $\beta$ is greater than zero:

$$ 
\frac{dl}{d\beta} = \frac{1}{2n} (-X')(Y - X \beta) + \lambda = 0
$$
$$
= (-X')(Y - X \beta) + n\lambda = 0
$$
$$
-X'Y + X'X \beta + n\lambda = 0
$$
$$
X'X \beta = X'Y - n\lambda
$$
$$
\beta = (X'X)^{-1}[X'Y - n\lambda] = X'Y - n\lambda
$$

